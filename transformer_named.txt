: SD3Transformer2DModel(
  (pos_embed): PatchEmbed(
    (proj): Conv2d(16, 1536, kernel_size=(2, 2), stride=(2, 2))
  )
  (time_text_embed): CombinedTimestepTextProjEmbeddings(
    (time_proj): Timesteps()
    (timestep_embedder): TimestepEmbedding(
      (linear_1): Linear(in_features=256, out_features=1536, bias=True)
      (act): SiLU()
      (linear_2): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (text_embedder): PixArtAlphaTextProjection(
      (linear_1): Linear(in_features=2048, out_features=1536, bias=True)
      (act_1): SiLU()
      (linear_2): Linear(in_features=1536, out_features=1536, bias=True)
    )
  )
  (context_embedder): Linear(in_features=4096, out_features=1536, bias=True)
  (transformer_blocks): ModuleList(
    (0-22): 23 x JointTransformerBlock(
      (norm1): AdaLayerNormZero(
        (silu): SiLU()
        (linear): Linear(in_features=1536, out_features=9216, bias=True)
        (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
      )
      (norm1_context): AdaLayerNormZero(
        (silu): SiLU()
        (linear): Linear(in_features=1536, out_features=9216, bias=True)
        (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
      )
      (attn): Attention(
        (to_q): Linear(in_features=1536, out_features=1536, bias=True)
        (to_k): Linear(in_features=1536, out_features=1536, bias=True)
        (to_v): Linear(in_features=1536, out_features=1536, bias=True)
        (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
        (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
        (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
        (to_out): ModuleList(
          (0): Linear(in_features=1536, out_features=1536, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
        (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
      )
      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
      (ff): FeedForward(
        (net): ModuleList(
          (0): GELU(
            (proj): Linear(in_features=1536, out_features=6144, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=6144, out_features=1536, bias=True)
        )
      )
      (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
      (ff_context): FeedForward(
        (net): ModuleList(
          (0): GELU(
            (proj): Linear(in_features=1536, out_features=6144, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=6144, out_features=1536, bias=True)
        )
      )
    )
    (23): JointTransformerBlock(
      (norm1): AdaLayerNormZero(
        (silu): SiLU()
        (linear): Linear(in_features=1536, out_features=9216, bias=True)
        (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
      )
      (norm1_context): AdaLayerNormContinuous(
        (silu): SiLU()
        (linear): Linear(in_features=1536, out_features=3072, bias=True)
        (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
      )
      (attn): Attention(
        (to_q): Linear(in_features=1536, out_features=1536, bias=True)
        (to_k): Linear(in_features=1536, out_features=1536, bias=True)
        (to_v): Linear(in_features=1536, out_features=1536, bias=True)
        (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
        (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
        (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
        (to_out): ModuleList(
          (0): Linear(in_features=1536, out_features=1536, bias=True)
          (1): Dropout(p=0.0, inplace=False)
        )
      )
      (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
      (ff): FeedForward(
        (net): ModuleList(
          (0): GELU(
            (proj): Linear(in_features=1536, out_features=6144, bias=True)
          )
          (1): Dropout(p=0.0, inplace=False)
          (2): Linear(in_features=6144, out_features=1536, bias=True)
        )
      )
    )
  )
  (norm_out): AdaLayerNormContinuous(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=3072, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (proj_out): Linear(in_features=1536, out_features=64, bias=True)
)
pos_embed: PatchEmbed(
  (proj): Conv2d(16, 1536, kernel_size=(2, 2), stride=(2, 2))
)
pos_embed.proj: Conv2d(16, 1536, kernel_size=(2, 2), stride=(2, 2))
time_text_embed: CombinedTimestepTextProjEmbeddings(
  (time_proj): Timesteps()
  (timestep_embedder): TimestepEmbedding(
    (linear_1): Linear(in_features=256, out_features=1536, bias=True)
    (act): SiLU()
    (linear_2): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (text_embedder): PixArtAlphaTextProjection(
    (linear_1): Linear(in_features=2048, out_features=1536, bias=True)
    (act_1): SiLU()
    (linear_2): Linear(in_features=1536, out_features=1536, bias=True)
  )
)
time_text_embed.time_proj: Timesteps()
time_text_embed.timestep_embedder: TimestepEmbedding(
  (linear_1): Linear(in_features=256, out_features=1536, bias=True)
  (act): SiLU()
  (linear_2): Linear(in_features=1536, out_features=1536, bias=True)
)
time_text_embed.timestep_embedder.linear_1: Linear(in_features=256, out_features=1536, bias=True)
time_text_embed.timestep_embedder.act: SiLU()
time_text_embed.timestep_embedder.linear_2: Linear(in_features=1536, out_features=1536, bias=True)
time_text_embed.text_embedder: PixArtAlphaTextProjection(
  (linear_1): Linear(in_features=2048, out_features=1536, bias=True)
  (act_1): SiLU()
  (linear_2): Linear(in_features=1536, out_features=1536, bias=True)
)
time_text_embed.text_embedder.linear_1: Linear(in_features=2048, out_features=1536, bias=True)
time_text_embed.text_embedder.act_1: SiLU()
time_text_embed.text_embedder.linear_2: Linear(in_features=1536, out_features=1536, bias=True)
context_embedder: Linear(in_features=4096, out_features=1536, bias=True)
transformer_blocks: ModuleList(
  (0-22): 23 x JointTransformerBlock(
    (norm1): AdaLayerNormZero(
      (silu): SiLU()
      (linear): Linear(in_features=1536, out_features=9216, bias=True)
      (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
    )
    (norm1_context): AdaLayerNormZero(
      (silu): SiLU()
      (linear): Linear(in_features=1536, out_features=9216, bias=True)
      (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
    )
    (attn): Attention(
      (to_q): Linear(in_features=1536, out_features=1536, bias=True)
      (to_k): Linear(in_features=1536, out_features=1536, bias=True)
      (to_v): Linear(in_features=1536, out_features=1536, bias=True)
      (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (to_out): ModuleList(
        (0): Linear(in_features=1536, out_features=1536, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
      (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
    )
    (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
    (ff): FeedForward(
      (net): ModuleList(
        (0): GELU(
          (proj): Linear(in_features=1536, out_features=6144, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=6144, out_features=1536, bias=True)
      )
    )
    (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
    (ff_context): FeedForward(
      (net): ModuleList(
        (0): GELU(
          (proj): Linear(in_features=1536, out_features=6144, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=6144, out_features=1536, bias=True)
      )
    )
  )
  (23): JointTransformerBlock(
    (norm1): AdaLayerNormZero(
      (silu): SiLU()
      (linear): Linear(in_features=1536, out_features=9216, bias=True)
      (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
    )
    (norm1_context): AdaLayerNormContinuous(
      (silu): SiLU()
      (linear): Linear(in_features=1536, out_features=3072, bias=True)
      (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
    )
    (attn): Attention(
      (to_q): Linear(in_features=1536, out_features=1536, bias=True)
      (to_k): Linear(in_features=1536, out_features=1536, bias=True)
      (to_v): Linear(in_features=1536, out_features=1536, bias=True)
      (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
      (to_out): ModuleList(
        (0): Linear(in_features=1536, out_features=1536, bias=True)
        (1): Dropout(p=0.0, inplace=False)
      )
    )
    (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
    (ff): FeedForward(
      (net): ModuleList(
        (0): GELU(
          (proj): Linear(in_features=1536, out_features=6144, bias=True)
        )
        (1): Dropout(p=0.0, inplace=False)
        (2): Linear(in_features=6144, out_features=1536, bias=True)
      )
    )
  )
)
transformer_blocks.0: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.0.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.0.norm1.silu: SiLU()
transformer_blocks.0.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.0.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.0.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.0.norm1_context.silu: SiLU()
transformer_blocks.0.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.0.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.0.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.0.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.0.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.0.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.0.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.0.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.0.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.0.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.0.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.0.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.0.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.0.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.0.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.0.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.0.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.0.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.0.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.0.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.0.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.0.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.0.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.0.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.0.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.0.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.0.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.1: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.1.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.1.norm1.silu: SiLU()
transformer_blocks.1.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.1.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.1.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.1.norm1_context.silu: SiLU()
transformer_blocks.1.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.1.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.1.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.1.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.1.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.1.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.1.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.1.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.1.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.1.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.1.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.1.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.1.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.1.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.1.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.1.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.1.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.1.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.1.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.1.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.1.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.1.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.1.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.1.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.1.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.1.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.1.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.2: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.2.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.2.norm1.silu: SiLU()
transformer_blocks.2.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.2.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.2.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.2.norm1_context.silu: SiLU()
transformer_blocks.2.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.2.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.2.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.2.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.2.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.2.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.2.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.2.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.2.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.2.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.2.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.2.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.2.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.2.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.2.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.2.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.2.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.2.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.2.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.2.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.2.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.2.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.2.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.2.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.2.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.2.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.2.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.3: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.3.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.3.norm1.silu: SiLU()
transformer_blocks.3.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.3.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.3.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.3.norm1_context.silu: SiLU()
transformer_blocks.3.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.3.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.3.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.3.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.3.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.3.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.3.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.3.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.3.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.3.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.3.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.3.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.3.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.3.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.3.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.3.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.3.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.3.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.3.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.3.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.3.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.3.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.3.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.3.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.3.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.3.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.3.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.4: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.4.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.4.norm1.silu: SiLU()
transformer_blocks.4.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.4.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.4.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.4.norm1_context.silu: SiLU()
transformer_blocks.4.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.4.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.4.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.4.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.4.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.4.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.4.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.4.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.4.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.4.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.4.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.4.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.4.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.4.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.4.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.4.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.4.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.4.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.4.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.4.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.4.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.4.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.4.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.4.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.4.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.4.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.4.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.5: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.5.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.5.norm1.silu: SiLU()
transformer_blocks.5.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.5.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.5.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.5.norm1_context.silu: SiLU()
transformer_blocks.5.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.5.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.5.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.5.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.5.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.5.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.5.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.5.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.5.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.5.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.5.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.5.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.5.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.5.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.5.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.5.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.5.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.5.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.5.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.5.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.5.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.5.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.5.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.5.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.5.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.5.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.5.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.6: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.6.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.6.norm1.silu: SiLU()
transformer_blocks.6.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.6.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.6.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.6.norm1_context.silu: SiLU()
transformer_blocks.6.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.6.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.6.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.6.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.6.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.6.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.6.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.6.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.6.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.6.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.6.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.6.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.6.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.6.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.6.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.6.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.6.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.6.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.6.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.6.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.6.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.6.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.6.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.6.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.6.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.6.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.6.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.7: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.7.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.7.norm1.silu: SiLU()
transformer_blocks.7.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.7.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.7.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.7.norm1_context.silu: SiLU()
transformer_blocks.7.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.7.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.7.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.7.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.7.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.7.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.7.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.7.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.7.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.7.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.7.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.7.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.7.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.7.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.7.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.7.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.7.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.7.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.7.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.7.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.7.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.7.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.7.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.7.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.7.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.7.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.7.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.8: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.8.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.8.norm1.silu: SiLU()
transformer_blocks.8.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.8.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.8.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.8.norm1_context.silu: SiLU()
transformer_blocks.8.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.8.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.8.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.8.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.8.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.8.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.8.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.8.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.8.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.8.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.8.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.8.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.8.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.8.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.8.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.8.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.8.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.8.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.8.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.8.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.8.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.8.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.8.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.8.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.8.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.8.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.8.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.9: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.9.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.9.norm1.silu: SiLU()
transformer_blocks.9.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.9.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.9.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.9.norm1_context.silu: SiLU()
transformer_blocks.9.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.9.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.9.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.9.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.9.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.9.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.9.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.9.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.9.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.9.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.9.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.9.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.9.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.9.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.9.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.9.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.9.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.9.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.9.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.9.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.9.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.9.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.9.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.9.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.9.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.9.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.9.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.10: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.10.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.10.norm1.silu: SiLU()
transformer_blocks.10.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.10.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.10.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.10.norm1_context.silu: SiLU()
transformer_blocks.10.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.10.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.10.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.10.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.10.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.10.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.10.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.10.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.10.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.10.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.10.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.10.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.10.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.10.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.10.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.10.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.10.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.10.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.10.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.10.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.10.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.10.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.10.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.10.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.10.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.10.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.10.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.11: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.11.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.11.norm1.silu: SiLU()
transformer_blocks.11.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.11.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.11.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.11.norm1_context.silu: SiLU()
transformer_blocks.11.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.11.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.11.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.11.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.11.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.11.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.11.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.11.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.11.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.11.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.11.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.11.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.11.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.11.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.11.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.11.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.11.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.11.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.11.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.11.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.11.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.11.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.11.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.11.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.11.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.11.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.11.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.12: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.12.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.12.norm1.silu: SiLU()
transformer_blocks.12.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.12.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.12.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.12.norm1_context.silu: SiLU()
transformer_blocks.12.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.12.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.12.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.12.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.12.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.12.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.12.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.12.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.12.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.12.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.12.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.12.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.12.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.12.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.12.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.12.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.12.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.12.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.12.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.12.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.12.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.12.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.12.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.12.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.12.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.12.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.12.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.13: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.13.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.13.norm1.silu: SiLU()
transformer_blocks.13.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.13.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.13.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.13.norm1_context.silu: SiLU()
transformer_blocks.13.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.13.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.13.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.13.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.13.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.13.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.13.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.13.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.13.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.13.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.13.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.13.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.13.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.13.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.13.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.13.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.13.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.13.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.13.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.13.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.13.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.13.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.13.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.13.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.13.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.13.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.13.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.14: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.14.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.14.norm1.silu: SiLU()
transformer_blocks.14.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.14.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.14.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.14.norm1_context.silu: SiLU()
transformer_blocks.14.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.14.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.14.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.14.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.14.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.14.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.14.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.14.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.14.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.14.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.14.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.14.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.14.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.14.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.14.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.14.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.14.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.14.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.14.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.14.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.14.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.14.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.14.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.14.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.14.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.14.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.14.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.15: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.15.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.15.norm1.silu: SiLU()
transformer_blocks.15.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.15.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.15.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.15.norm1_context.silu: SiLU()
transformer_blocks.15.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.15.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.15.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.15.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.15.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.15.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.15.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.15.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.15.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.15.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.15.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.15.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.15.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.15.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.15.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.15.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.15.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.15.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.15.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.15.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.15.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.15.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.15.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.15.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.15.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.15.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.15.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.16: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.16.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.16.norm1.silu: SiLU()
transformer_blocks.16.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.16.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.16.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.16.norm1_context.silu: SiLU()
transformer_blocks.16.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.16.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.16.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.16.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.16.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.16.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.16.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.16.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.16.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.16.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.16.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.16.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.16.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.16.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.16.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.16.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.16.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.16.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.16.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.16.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.16.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.16.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.16.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.16.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.16.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.16.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.16.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.17: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.17.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.17.norm1.silu: SiLU()
transformer_blocks.17.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.17.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.17.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.17.norm1_context.silu: SiLU()
transformer_blocks.17.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.17.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.17.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.17.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.17.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.17.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.17.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.17.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.17.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.17.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.17.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.17.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.17.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.17.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.17.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.17.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.17.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.17.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.17.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.17.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.17.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.17.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.17.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.17.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.17.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.17.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.17.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.18: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.18.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.18.norm1.silu: SiLU()
transformer_blocks.18.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.18.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.18.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.18.norm1_context.silu: SiLU()
transformer_blocks.18.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.18.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.18.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.18.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.18.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.18.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.18.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.18.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.18.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.18.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.18.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.18.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.18.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.18.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.18.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.18.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.18.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.18.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.18.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.18.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.18.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.18.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.18.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.18.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.18.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.18.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.18.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.19: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.19.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.19.norm1.silu: SiLU()
transformer_blocks.19.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.19.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.19.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.19.norm1_context.silu: SiLU()
transformer_blocks.19.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.19.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.19.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.19.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.19.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.19.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.19.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.19.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.19.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.19.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.19.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.19.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.19.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.19.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.19.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.19.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.19.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.19.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.19.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.19.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.19.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.19.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.19.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.19.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.19.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.19.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.19.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.20: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.20.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.20.norm1.silu: SiLU()
transformer_blocks.20.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.20.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.20.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.20.norm1_context.silu: SiLU()
transformer_blocks.20.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.20.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.20.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.20.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.20.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.20.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.20.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.20.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.20.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.20.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.20.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.20.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.20.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.20.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.20.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.20.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.20.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.20.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.20.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.20.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.20.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.20.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.20.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.20.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.20.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.20.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.20.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.21: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.21.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.21.norm1.silu: SiLU()
transformer_blocks.21.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.21.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.21.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.21.norm1_context.silu: SiLU()
transformer_blocks.21.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.21.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.21.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.21.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.21.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.21.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.21.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.21.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.21.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.21.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.21.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.21.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.21.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.21.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.21.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.21.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.21.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.21.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.21.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.21.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.21.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.21.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.21.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.21.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.21.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.21.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.21.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.22: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
    (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
  (norm2_context): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff_context): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.22.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.22.norm1.silu: SiLU()
transformer_blocks.22.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.22.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.22.norm1_context: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.22.norm1_context.silu: SiLU()
transformer_blocks.22.norm1_context.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.22.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.22.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
  (to_add_out): Linear(in_features=1536, out_features=1536, bias=True)
)
transformer_blocks.22.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.22.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.22.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.22.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.22.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.22.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.22.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.22.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.22.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.22.attn.to_add_out: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.22.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.22.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.22.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.22.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.22.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.22.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.22.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.22.norm2_context: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.22.ff_context: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.22.ff_context.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.22.ff_context.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.22.ff_context.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.22.ff_context.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.22.ff_context.net.2: Linear(in_features=6144, out_features=1536, bias=True)
transformer_blocks.23: JointTransformerBlock(
  (norm1): AdaLayerNormZero(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=9216, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (norm1_context): AdaLayerNormContinuous(
    (silu): SiLU()
    (linear): Linear(in_features=1536, out_features=3072, bias=True)
    (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  )
  (attn): Attention(
    (to_q): Linear(in_features=1536, out_features=1536, bias=True)
    (to_k): Linear(in_features=1536, out_features=1536, bias=True)
    (to_v): Linear(in_features=1536, out_features=1536, bias=True)
    (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
    (to_out): ModuleList(
      (0): Linear(in_features=1536, out_features=1536, bias=True)
      (1): Dropout(p=0.0, inplace=False)
    )
  )
  (norm2): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
  (ff): FeedForward(
    (net): ModuleList(
      (0): GELU(
        (proj): Linear(in_features=1536, out_features=6144, bias=True)
      )
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=6144, out_features=1536, bias=True)
    )
  )
)
transformer_blocks.23.norm1: AdaLayerNormZero(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=9216, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.23.norm1.silu: SiLU()
transformer_blocks.23.norm1.linear: Linear(in_features=1536, out_features=9216, bias=True)
transformer_blocks.23.norm1.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.23.norm1_context: AdaLayerNormContinuous(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=3072, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
transformer_blocks.23.norm1_context.silu: SiLU()
transformer_blocks.23.norm1_context.linear: Linear(in_features=1536, out_features=3072, bias=True)
transformer_blocks.23.norm1_context.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.23.attn: Attention(
  (to_q): Linear(in_features=1536, out_features=1536, bias=True)
  (to_k): Linear(in_features=1536, out_features=1536, bias=True)
  (to_v): Linear(in_features=1536, out_features=1536, bias=True)
  (add_k_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_v_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (add_q_proj): Linear(in_features=1536, out_features=1536, bias=True)
  (to_out): ModuleList(
    (0): Linear(in_features=1536, out_features=1536, bias=True)
    (1): Dropout(p=0.0, inplace=False)
  )
)
transformer_blocks.23.attn.to_q: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.23.attn.to_k: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.23.attn.to_v: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.23.attn.add_k_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.23.attn.add_v_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.23.attn.add_q_proj: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.23.attn.to_out: ModuleList(
  (0): Linear(in_features=1536, out_features=1536, bias=True)
  (1): Dropout(p=0.0, inplace=False)
)
transformer_blocks.23.attn.to_out.0: Linear(in_features=1536, out_features=1536, bias=True)
transformer_blocks.23.attn.to_out.1: Dropout(p=0.0, inplace=False)
transformer_blocks.23.norm2: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
transformer_blocks.23.ff: FeedForward(
  (net): ModuleList(
    (0): GELU(
      (proj): Linear(in_features=1536, out_features=6144, bias=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=6144, out_features=1536, bias=True)
  )
)
transformer_blocks.23.ff.net: ModuleList(
  (0): GELU(
    (proj): Linear(in_features=1536, out_features=6144, bias=True)
  )
  (1): Dropout(p=0.0, inplace=False)
  (2): Linear(in_features=6144, out_features=1536, bias=True)
)
transformer_blocks.23.ff.net.0: GELU(
  (proj): Linear(in_features=1536, out_features=6144, bias=True)
)
transformer_blocks.23.ff.net.0.proj: Linear(in_features=1536, out_features=6144, bias=True)
transformer_blocks.23.ff.net.1: Dropout(p=0.0, inplace=False)
transformer_blocks.23.ff.net.2: Linear(in_features=6144, out_features=1536, bias=True)
norm_out: AdaLayerNormContinuous(
  (silu): SiLU()
  (linear): Linear(in_features=1536, out_features=3072, bias=True)
  (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
)
norm_out.silu: SiLU()
norm_out.linear: Linear(in_features=1536, out_features=3072, bias=True)
norm_out.norm: LayerNorm((1536,), eps=1e-06, elementwise_affine=False)
proj_out: Linear(in_features=1536, out_features=64, bias=True)
